{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152a4d32",
   "metadata": {},
   "source": [
    "# Process original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad71b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ae1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_dirs(root_dir):\n",
    "    person_dirs = []\n",
    "    for dirpath, dirnames, _ in os.walk(root_dir):\n",
    "        for dirname in dirnames:\n",
    "            if dirname.startswith(\"person\"):\n",
    "                person_dirs.append(os.path.join(dirpath, dirname))\n",
    "    return sorted(person_dirs)\n",
    "\n",
    "def get_meta(person_dir: str):\n",
    "    person_id = os.path.basename(person_dir)[-3:]\n",
    "    meta_path = f\"/fast/dataset/baby-motion/dataset/00_sequence/person{person_id}/{person_id}_meta.csv\"\n",
    "    gender = \"\"\n",
    "    age = None\n",
    "    try:\n",
    "        meta_df = pd.read_csv(meta_path, header=None, names=[\"key\", \"value\"], encoding='utf-8-sig')\n",
    "        meta_df[\"key\"] = meta_df[\"key\"].str.strip().str.lower()\n",
    "        meta_df[\"value\"] = meta_df[\"value\"].astype(str).str.strip()\n",
    "        data = dict(zip(meta_df[\"key\"], meta_df[\"value\"]))\n",
    "        gender = data.get(\"gender\", \"\").lower()\n",
    "        age_match = re.search(r'\\d+', data.get(\"age\", \"\"))\n",
    "        if age_match:\n",
    "            age = int(age_match.group())\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Failed to read meta file: {e}\")\n",
    "    return person_id, gender, age\n",
    "\n",
    "def get_label_in_csv(person_dir: str):\n",
    "    labels = []\n",
    "    person_id, gender, age = get_meta(person_dir)\n",
    "    label_path = os.path.join(person_dir, f\"{person_id}_label.csv\")\n",
    "    data_path = os.path.join(person_dir, f\"{person_id}.csv\")\n",
    "    try:\n",
    "        df = pd.read_csv(label_path, header=None)\n",
    "    except:\n",
    "        return [], data_path\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            if ' ' in row[0]:  # (datetime_start, datetime_end, action)\n",
    "                t0 = datetime.strptime(row[0], \"%Y/%m/%d %H:%M:%S\")\n",
    "                t1 = datetime.strptime(row[1], \"%Y/%m/%d %H:%M:%S\")\n",
    "                action = row[2]\n",
    "            else:  # (date, t_start, t_end, action)\n",
    "                t0 = datetime.strptime(row[0] + \" \" + row[1], \"%Y/%m/%d %H:%M:%S\")\n",
    "                t1 = datetime.strptime(row[0] + \" \" + row[2], \"%Y/%m/%d %H:%M:%S\")\n",
    "                action = row[3]\n",
    "            if not action:\n",
    "                continue\n",
    "            label = {\n",
    "                \"person_id\": person_id,\n",
    "                \"gender\": gender,\n",
    "                \"age\": age,\n",
    "                \"dt_start\": t0.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"dt_end\": t1.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"dur\": (t1 - t0).total_seconds(),\n",
    "                \"action\": action\n",
    "            }\n",
    "            labels.append(label)\n",
    "        except:\n",
    "            continue\n",
    "    return labels, data_path\n",
    "\n",
    "def get_label_in_filename(person_dir: str, max_gap_seconds: int = 60):\n",
    "    person_id, gender, age = get_meta(person_dir)\n",
    "    data_path = os.path.join(person_dir, f\"{person_id}.csv\")\n",
    "    action = os.path.basename(os.path.dirname(person_dir))\n",
    "    labels = []\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, encoding=\"utf-8-sig\")\n",
    "        time_series = df.iloc[:, 0].dropna().astype(str)\n",
    "        timestamps = [datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S.%f\") for ts in time_series]\n",
    "        start_idx = 0\n",
    "        for i in range(1, len(timestamps)):\n",
    "            gap = (timestamps[i] - timestamps[i - 1]).total_seconds()\n",
    "            if gap > max_gap_seconds:\n",
    "                t_start = timestamps[start_idx].replace(microsecond=0)\n",
    "                t_end = timestamps[i - 1]\n",
    "                if t_end.microsecond > 0:\n",
    "                    t_end = t_end.replace(microsecond=0) + timedelta(seconds=1)\n",
    "                else:\n",
    "                    t_end = t_end.replace(microsecond=0)\n",
    "                labels.append({\n",
    "                    \"person_id\": person_id,\n",
    "                    \"gender\": gender,\n",
    "                    \"age\": age,\n",
    "                    \"dt_start\": t_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"dt_end\": t_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"dur\": (t_end - t_start).total_seconds(),\n",
    "                    \"action\": action,\n",
    "                })\n",
    "                start_idx = i\n",
    "        t_start = timestamps[start_idx].replace(microsecond=0)\n",
    "        t_end = timestamps[-1]\n",
    "        if t_end.microsecond > 0:\n",
    "            t_end = t_end.replace(microsecond=0) + timedelta(seconds=1)\n",
    "        else:\n",
    "            t_end = t_end.replace(microsecond=0)\n",
    "        if (t_end - t_start).total_seconds() > 1:\n",
    "            labels.append({\n",
    "                # \"person_id\": person_id,\n",
    "                \"gender\": gender,\n",
    "                \"age\": age,\n",
    "                # \"dt_start\": t_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                # \"dt_end\": t_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"dur\": (t_end - t_start).total_seconds(),\n",
    "                \"action\": action,\n",
    "            })\n",
    "        return labels, data_path\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Failed to parse file: {e}\")\n",
    "    return labels, data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53273f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 11:39:37',\n",
       "   'dt_end': '2022-08-28 11:39:39',\n",
       "   'dur': 2.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 11:40:03',\n",
       "   'dt_end': '2022-08-28 11:40:27',\n",
       "   'dur': 24.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 11:41:42',\n",
       "   'dt_end': '2022-08-28 11:42:00',\n",
       "   'dur': 18.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 11:42:20',\n",
       "   'dt_end': '2022-08-28 12:12:09',\n",
       "   'dur': 1789.0,\n",
       "   'action': 'face-down'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 12:12:10',\n",
       "   'dt_end': '2022-08-28 12:12:22',\n",
       "   'dur': 12.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 12:12:22',\n",
       "   'dt_end': '2022-08-28 13:07:19',\n",
       "   'dur': 3297.0,\n",
       "   'action': 'face-up'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 13:07:20',\n",
       "   'dt_end': '2022-08-28 13:07:23',\n",
       "   'dur': 3.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 13:07:37',\n",
       "   'dt_end': '2022-08-28 13:07:46',\n",
       "   'dur': 9.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 13:10:18',\n",
       "   'dt_end': '2022-08-28 13:10:22',\n",
       "   'dur': 4.0,\n",
       "   'action': 'crawl'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 11:33:01',\n",
       "   'dt_end': '2022-09-03 12:13:09',\n",
       "   'dur': 2408.0,\n",
       "   'action': 'face-down'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 12:13:09',\n",
       "   'dt_end': '2022-09-03 12:13:16',\n",
       "   'dur': 7.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 12:13:16',\n",
       "   'dt_end': '2022-09-03 12:23:14',\n",
       "   'dur': 598.0,\n",
       "   'action': 'face-up'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 12:23:14',\n",
       "   'dt_end': '2022-09-03 12:23:21',\n",
       "   'dur': 7.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 12:23:21',\n",
       "   'dt_end': '2022-09-03 13:13:50',\n",
       "   'dur': 3029.0,\n",
       "   'action': 'face-side'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 13:13:51',\n",
       "   'dt_end': '2022-09-03 13:13:55',\n",
       "   'dur': 4.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 13:13:55',\n",
       "   'dt_end': '2022-09-03 13:14:13',\n",
       "   'dur': 18.0,\n",
       "   'action': 'face-up'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 13:14:14',\n",
       "   'dt_end': '2022-09-03 13:14:19',\n",
       "   'dur': 5.0,\n",
       "   'action': 'roll-over'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:07:30',\n",
       "   'dt_end': '2022-09-03 19:07:44',\n",
       "   'dur': 14.0,\n",
       "   'action': 'hold-vertical'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:07:45',\n",
       "   'dt_end': '2022-09-03 19:07:48',\n",
       "   'dur': 3.0,\n",
       "   'action': 'hold-vertical'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:07:49',\n",
       "   'dt_end': '2022-09-03 19:08:00',\n",
       "   'dur': 11.0,\n",
       "   'action': 'hold-vertical'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:08:01',\n",
       "   'dt_end': '2022-09-03 19:08:14',\n",
       "   'dur': 13.0,\n",
       "   'action': 'hold-vertical'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:08:25',\n",
       "   'dt_end': '2022-09-03 19:08:35',\n",
       "   'dur': 10.0,\n",
       "   'action': 'hold-horizontal'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:08:40',\n",
       "   'dt_end': '2022-09-03 19:08:42',\n",
       "   'dur': 2.0,\n",
       "   'action': 'hold-horizontal'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:08:43',\n",
       "   'dt_end': '2022-09-03 19:08:59',\n",
       "   'dur': 16.0,\n",
       "   'action': 'hold-horizontal'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:09:07',\n",
       "   'dt_end': '2022-09-03 19:09:24',\n",
       "   'dur': 17.0,\n",
       "   'action': 'hold-horizontal'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:09:51',\n",
       "   'dt_end': '2022-09-03 19:09:52',\n",
       "   'dur': 1.0,\n",
       "   'action': 'walk'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:10:10',\n",
       "   'dt_end': '2022-09-03 19:10:22',\n",
       "   'dur': 12.0,\n",
       "   'action': 'stand'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:10:40',\n",
       "   'dt_end': '2022-09-03 19:10:48',\n",
       "   'dur': 8.0,\n",
       "   'action': 'walk'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:10:57',\n",
       "   'dt_end': '2022-09-03 19:11:14',\n",
       "   'dur': 17.0,\n",
       "   'action': 'sit-high-chair'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:11:22',\n",
       "   'dt_end': '2022-09-03 19:11:41',\n",
       "   'dur': 19.0,\n",
       "   'action': 'walk'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:11:42',\n",
       "   'dt_end': '2022-09-03 19:11:47',\n",
       "   'dur': 5.0,\n",
       "   'action': 'stand'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:11:56',\n",
       "   'dt_end': '2022-09-03 19:12:08',\n",
       "   'dur': 12.0,\n",
       "   'action': 'crawl'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:12:17',\n",
       "   'dt_end': '2022-09-03 19:12:20',\n",
       "   'dur': 3.0,\n",
       "   'action': 'walk'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:12:23',\n",
       "   'dt_end': '2022-09-03 19:12:29',\n",
       "   'dur': 6.0,\n",
       "   'action': 'walk'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:12:54',\n",
       "   'dt_end': '2022-09-03 19:13:29',\n",
       "   'dur': 35.0,\n",
       "   'action': 'sit-floor'}],\n",
       " '/fast/dataset/baby-motion/dataset/00_sequence/person001/001.csv')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_in_csv(\"/fast/dataset/baby-motion/dataset/00_sequence/person001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d277f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-08-28 13:10:18',\n",
       "   'dt_end': '2022-08-28 13:10:22',\n",
       "   'dur': 4.0,\n",
       "   'action': 'crawl'},\n",
       "  {'person_id': '001',\n",
       "   'gender': 'female',\n",
       "   'age': 16,\n",
       "   'dt_start': '2022-09-03 19:11:56',\n",
       "   'dt_end': '2022-09-03 19:12:08',\n",
       "   'dur': 12.0,\n",
       "   'action': 'crawl'}],\n",
       " '/fast/dataset/baby-motion/dataset/01_move/crawl/person001/001.csv')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_in_filename(\"/fast/dataset/baby-motion/dataset/01_move/crawl/person001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cab570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/83 [00:03<00:46,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Failed to read meta file: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/00_sequence/person006/006_meta.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 18/83 [00:04<00:05, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Failed to read meta file: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/00_sequence/person006/006_meta.csv'\n",
      "[Warning] Failed to read meta file: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/00_sequence/person006/006_meta.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 37/83 [00:05<00:01, 29.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Failed to read meta file: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/00_sequence/person006/006_meta.csv'\n",
      "[Warning] Failed to parse file: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/03_hold/hold-horizontal/person002/002.csv'\n",
      "[Warning] Failed to process person dir: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/03_hold/hold-horizontal/person002/002.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 49/83 [00:05<00:01, 27.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Failed to read meta file: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/00_sequence/person006/006_meta.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 59/83 [00:05<00:00, 25.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Failed to read meta file: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/00_sequence/person006/006_meta.csv'\n",
      "[Warning] Failed to read meta file: [Errno 2] No such file or directory: '/fast/dataset/baby-motion/dataset/00_sequence/person006/006_meta.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  8.37it/s]\n"
     ]
    }
   ],
   "source": [
    "input_dirs = find_all_dirs(root_dir=\"/fast/dataset/baby-motion/dataset\")\n",
    "data_save_dir = \"/fast/workspace/robinson/CodeSource/babycare/data_origin/sequence\"\n",
    "label_save_dir = \"/fast/workspace/robinson/CodeSource/babycare/data_origin/label\"\n",
    "os.makedirs(data_save_dir, exist_ok=True)\n",
    "os.makedirs(label_save_dir, exist_ok=True)\n",
    "\n",
    "index = 0\n",
    "for person_dir in tqdm(input_dirs):\n",
    "    try:\n",
    "        labels, data_path = get_label_in_csv(person_dir) if \"sequence\" in person_dir else get_label_in_filename(person_dir)\n",
    "        data_df = pd.read_csv(data_path)\n",
    "        data_df['datetime'] = pd.to_datetime(data_df['datetime'], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "        data_df = data_df.dropna(subset=['datetime'])\n",
    "        for label in labels:\n",
    "            try:\n",
    "                t_start = pd.to_datetime(label['dt_start'])\n",
    "                t_end = pd.to_datetime(label['dt_end'])\n",
    "                segment = data_df[(data_df['datetime'] >= t_start) & (data_df['datetime'] <= t_end)]\n",
    "                if segment.empty:\n",
    "                    continue\n",
    "                sequence_filename = f\"{index:06d}.csv\"\n",
    "                segment[['accel_x', 'accel_y', 'accel_z']].to_csv(os.path.join(data_save_dir, sequence_filename), index=False)\n",
    "                label_filename = f\"{index:06d}_label.csv\"\n",
    "                pd.DataFrame([label]).to_csv(os.path.join(label_save_dir, label_filename), index=False)\n",
    "                index += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Failed to process segment: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Failed to process person dir: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38067fc5",
   "metadata": {},
   "source": [
    "# Check dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972a6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "origin_sequence_dir = './data_origin/sequence'\n",
    "origin_label_dir = './data_origin/label'\n",
    "aug_sequence_dir = './data_aug/ChatGPT-o4-instructed/sequence'\n",
    "aug_label_dir = './data_aug/ChatGPT-o4-instructed/label'\n",
    "\n",
    "sampling_interval = 0.15  # s/frame\n",
    "tolerance = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files passed.\n"
     ]
    }
   ],
   "source": [
    "# check if the fromat correct\n",
    "\n",
    "errors = []\n",
    "sequence_files = sorted(f for f in os.listdir(aug_sequence_dir) if f.endswith('.csv'))\n",
    "label_files = sorted(f for f in os.listdir(aug_label_dir) if f.endswith('.csv'))\n",
    "\n",
    "for seq_file in sequence_files:\n",
    "    base = os.path.splitext(seq_file)[0]\n",
    "    label_file = f\"{base}_label.csv\"\n",
    "\n",
    "    seq_path = os.path.join(aug_sequence_dir, seq_file)\n",
    "    label_path = os.path.join(aug_label_dir, label_file)\n",
    "\n",
    "    # check if label exist\n",
    "    if not os.path.exists(label_path):\n",
    "        errors.append(f\"Label file missing for {seq_file}\")\n",
    "        continue\n",
    "\n",
    "    # check sequence format\n",
    "    try:\n",
    "        df_seq = pd.read_csv(seq_path)\n",
    "        if df_seq.shape[1] != 3:\n",
    "            errors.append(f\"{seq_file} should have 3 columns, found {df_seq.shape[1]}\")\n",
    "        if df_seq.isnull().any().any():\n",
    "            errors.append(f\"{seq_file} contains NaN values\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Failed to read {seq_file}: {e}\")\n",
    "\n",
    "    # check label format\n",
    "    try:\n",
    "        df_label = pd.read_csv(label_path)\n",
    "        expected_cols = ['gender', 'age', 'action', 'dur']\n",
    "        if list(df_label.columns) != expected_cols:\n",
    "            errors.append(f\"{label_file} header mismatch. Expected {expected_cols}, got {list(df_label.columns)}\")\n",
    "        if df_label.shape[0] != 1:\n",
    "            errors.append(f\"{label_file} should contain exactly 1 row, found {df_label.shape[0]}\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Failed to read {label_file}: {e}\")\n",
    "if errors:\n",
    "    print(\"\\n\".join(errors))\n",
    "else:\n",
    "    print(\"All files passed the format check.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28077dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These files' duration in label is implausible：\n",
      " - A00001: num_frames=55, duration=8.10s, expexted duration=8.25s, delta=0.15s\n",
      " - A00002: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00004: num_frames=56, duration=8.25s, expexted duration=8.40s, delta=0.15s\n",
      " - A00015: num_frames=45, duration=6.60s, expexted duration=6.75s, delta=0.15s\n",
      " - A00019: num_frames=50, duration=7.35s, expexted duration=7.50s, delta=0.15s\n",
      " - A00022: num_frames=62, duration=9.00s, expexted duration=9.30s, delta=0.30s\n",
      " - A00023: num_frames=56, duration=8.25s, expexted duration=8.40s, delta=0.15s\n",
      " - A00026: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00029: num_frames=58, duration=9.00s, expexted duration=8.70s, delta=0.30s\n",
      " - A00030: num_frames=50, duration=7.20s, expexted duration=7.50s, delta=0.30s\n",
      " - A00034: num_frames=51, duration=7.80s, expexted duration=7.65s, delta=0.15s\n",
      " - A00036: num_frames=48, duration=7.50s, expexted duration=7.20s, delta=0.30s\n",
      " - A00039: num_frames=45, duration=6.90s, expexted duration=6.75s, delta=0.15s\n",
      " - A00041: num_frames=56, duration=8.10s, expexted duration=8.40s, delta=0.30s\n",
      " - A00043: num_frames=59, duration=8.40s, expexted duration=8.85s, delta=0.45s\n",
      " - A00045: num_frames=59, duration=8.70s, expexted duration=8.85s, delta=0.15s\n",
      " - A00048: num_frames=56, duration=8.25s, expexted duration=8.40s, delta=0.15s\n",
      " - A00049: num_frames=54, duration=7.65s, expexted duration=8.10s, delta=0.45s\n",
      " - A00050: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00051: num_frames=60, duration=8.70s, expexted duration=9.00s, delta=0.30s\n",
      " - A00052: num_frames=54, duration=7.80s, expexted duration=8.10s, delta=0.30s\n",
      " - A00053: num_frames=58, duration=8.40s, expexted duration=8.70s, delta=0.30s\n",
      " - A00054: num_frames=64, duration=8.85s, expexted duration=9.60s, delta=0.75s\n",
      " - A00055: num_frames=57, duration=8.25s, expexted duration=8.55s, delta=0.30s\n",
      " - A00056: num_frames=57, duration=8.10s, expexted duration=8.55s, delta=0.45s\n",
      " - A00057: num_frames=55, duration=7.95s, expexted duration=8.25s, delta=0.30s\n",
      " - A00059: num_frames=60, duration=8.70s, expexted duration=9.00s, delta=0.30s\n",
      " - A00070: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00071: num_frames=71, duration=10.05s, expexted duration=10.65s, delta=0.60s\n",
      " - A00072: num_frames=73, duration=10.50s, expexted duration=10.95s, delta=0.45s\n",
      " - A00073: num_frames=69, duration=10.05s, expexted duration=10.35s, delta=0.30s\n",
      " - A00074: num_frames=66, duration=10.05s, expexted duration=9.90s, delta=0.15s\n",
      " - A00075: num_frames=68, duration=10.50s, expexted duration=10.20s, delta=0.30s\n",
      " - A00077: num_frames=65, duration=10.05s, expexted duration=9.75s, delta=0.30s\n",
      " - A00078: num_frames=66, duration=10.05s, expexted duration=9.90s, delta=0.15s\n",
      " - A00079: num_frames=65, duration=10.05s, expexted duration=9.75s, delta=0.30s\n",
      " - A00081: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00082: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00085: num_frames=59, duration=9.00s, expexted duration=8.85s, delta=0.15s\n",
      " - A00087: num_frames=59, duration=9.00s, expexted duration=8.85s, delta=0.15s\n",
      " - A00088: num_frames=59, duration=9.00s, expexted duration=8.85s, delta=0.15s\n",
      " - A00089: num_frames=58, duration=9.00s, expexted duration=8.70s, delta=0.30s\n",
      " - A00091: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00093: num_frames=62, duration=9.00s, expexted duration=9.30s, delta=0.30s\n",
      " - A00094: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00098: num_frames=59, duration=9.00s, expexted duration=8.85s, delta=0.15s\n",
      " - A00099: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00100: num_frames=55, duration=8.10s, expexted duration=8.25s, delta=0.15s\n",
      " - A00101: num_frames=55, duration=8.10s, expexted duration=8.25s, delta=0.15s\n",
      " - A00102: num_frames=53, duration=8.10s, expexted duration=7.95s, delta=0.15s\n",
      " - A00103: num_frames=55, duration=8.10s, expexted duration=8.25s, delta=0.15s\n",
      " - A00104: num_frames=53, duration=8.10s, expexted duration=7.95s, delta=0.15s\n",
      " - A00105: num_frames=55, duration=8.10s, expexted duration=8.25s, delta=0.15s\n",
      " - A00107: num_frames=55, duration=8.10s, expexted duration=8.25s, delta=0.15s\n",
      " - A00108: num_frames=55, duration=8.10s, expexted duration=8.25s, delta=0.15s\n",
      " - A00109: num_frames=55, duration=8.10s, expexted duration=8.25s, delta=0.15s\n",
      " - A00111: num_frames=53, duration=7.65s, expexted duration=7.95s, delta=0.30s\n",
      " - A00112: num_frames=62, duration=9.00s, expexted duration=9.30s, delta=0.30s\n",
      " - A00113: num_frames=57, duration=8.25s, expexted duration=8.55s, delta=0.30s\n",
      " - A00114: num_frames=58, duration=8.40s, expexted duration=8.70s, delta=0.30s\n",
      " - A00115: num_frames=57, duration=8.10s, expexted duration=8.55s, delta=0.45s\n",
      " - A00119: num_frames=60, duration=8.70s, expexted duration=9.00s, delta=0.30s\n",
      " - A00123: num_frames=58, duration=8.25s, expexted duration=8.70s, delta=0.45s\n",
      " - A00126: num_frames=66, duration=9.30s, expexted duration=9.90s, delta=0.60s\n",
      " - A00127: num_frames=61, duration=9.30s, expexted duration=9.15s, delta=0.15s\n",
      " - A00128: num_frames=65, duration=9.45s, expexted duration=9.75s, delta=0.30s\n",
      " - A00129: num_frames=60, duration=9.60s, expexted duration=9.00s, delta=0.60s\n",
      " - A00130: num_frames=38, duration=5.55s, expexted duration=5.70s, delta=0.15s\n",
      " - A00136: num_frames=49, duration=7.05s, expexted duration=7.35s, delta=0.30s\n",
      " - A00138: num_frames=40, duration=5.85s, expexted duration=6.00s, delta=0.15s\n",
      " - A00140: num_frames=61, duration=9.00s, expexted duration=9.15s, delta=0.15s\n",
      " - A00141: num_frames=59, duration=9.00s, expexted duration=8.85s, delta=0.15s\n",
      " - A00142: num_frames=58, duration=9.00s, expexted duration=8.70s, delta=0.30s\n",
      " - A00143: num_frames=58, duration=9.00s, expexted duration=8.70s, delta=0.30s\n",
      " - A00144: num_frames=57, duration=9.00s, expexted duration=8.55s, delta=0.45s\n",
      " - A00145: num_frames=57, duration=9.00s, expexted duration=8.55s, delta=0.45s\n",
      " - A00146: num_frames=57, duration=9.00s, expexted duration=8.55s, delta=0.45s\n",
      " - A00147: num_frames=57, duration=9.00s, expexted duration=8.55s, delta=0.45s\n",
      " - A00148: num_frames=57, duration=9.00s, expexted duration=8.55s, delta=0.45s\n",
      " - A00149: num_frames=57, duration=9.00s, expexted duration=8.55s, delta=0.45s\n"
     ]
    }
   ],
   "source": [
    "# check if the duration plausible\n",
    "\n",
    "errors = []\n",
    "for filename in sorted(os.listdir(aug_sequence_dir)):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    base_id = os.path.splitext(filename)[0]  # A00001\n",
    "    sequence_path = os.path.join(aug_sequence_dir, filename)\n",
    "    label_path = os.path.join(aug_label_dir, f\"{base_id}_label.csv\")\n",
    "\n",
    "    try:\n",
    "        seq_df = pd.read_csv(sequence_path)\n",
    "        label_df = pd.read_csv(label_path)\n",
    "\n",
    "        num_frames = len(seq_df)\n",
    "        duration = float(label_df.iloc[0]['dur'])\n",
    "\n",
    "        expected_duration = num_frames * sampling_interval\n",
    "        delta = abs(expected_duration - duration)\n",
    "\n",
    "        if delta > tolerance:\n",
    "            errors.append((base_id, num_frames, duration, expected_duration, delta))\n",
    "    except Exception as e:\n",
    "        print(f\"can't load data of {base_id}: {e}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"These files' duration in label is implausible：\")\n",
    "    for base_id, frames, label_dur, expected_dur, delta in errors:\n",
    "        print(f\" - {base_id}: num_frames={frames}, duration={label_dur:.2f}s, expexted duration={expected_dur:.2f}s, delta={delta:.2f}s\")\n",
    "else:\n",
    "    print(\"All duration in labels are plausible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d536050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "action classes in origin dataset： ['baby-food', 'bottle', 'breast', 'crawl', 'face-down', 'face-side', 'face-up', 'hold-horizontal', 'hold-vertical', 'piggyback', 'roll-over', 'sit-floor', 'sit-high-chair', 'sit-low-chair', 'stand', 'walk']\n",
      "action classes in augmented dataset： ['baby-food', 'bottle', 'crawl', 'face-down', 'face-side', 'face-up', 'hold-horizontal', 'hold-vertical', 'piggyback', 'roll-over', 'sit-floor', 'sit-high-chair', 'sit-low-chair', 'stand', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# detect action classes in labels\n",
    "\n",
    "def collect_action_classes(label_root):\n",
    "    all_actions = set()\n",
    "    for filename in sorted(os.listdir(label_root)):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(label_root, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if 'action' not in df.columns:\n",
    "                    print(f\"no action feature for {filename}\")\n",
    "                    continue\n",
    "                actions = df['action'].dropna().unique()\n",
    "                all_actions.update(actions)\n",
    "            except Exception as e:\n",
    "                print(f\"fail to load {filename}: {e}\")\n",
    "    return sorted(list(all_actions))\n",
    "\n",
    "original_actions = collect_action_classes(origin_label_dir)\n",
    "augmented_actions = collect_action_classes(aug_label_dir)\n",
    "\n",
    "print(\"\\naction classes in origin dataset：\", original_actions)\n",
    "print(\"action classes in augmented dataset：\", augmented_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5278090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finded out 28 samples for action class 'crawl':\n",
      "['000008', '000031', '000042', '000044', '000050', '000052', '000054', '000056', '000075', '000078', '000088', '000090', '000093', '000182', '000183', '000370', '000371', '000372', '000373', '000374', '000375', '000376', '000377', '000378', '000379', '000380', '000381', '000382']\n"
     ]
    }
   ],
   "source": [
    "# detect num of data sample for specific action class\n",
    "\n",
    "target_action = 'crawl' \n",
    "\n",
    "matched_filenames = []\n",
    "for filename in sorted(os.listdir(origin_label_dir)):\n",
    "    if filename.endswith('_label.csv'):\n",
    "        filepath = os.path.join(origin_label_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        if target_action in df['action'].values:\n",
    "            file_id = filename.replace('_label.csv', '')\n",
    "            matched_filenames.append(file_id)\n",
    "\n",
    "print(f\"finded out {len(matched_filenames)} samples for action class '{target_action}':\")\n",
    "print(matched_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a62a21",
   "metadata": {},
   "source": [
    "# Split train-set and validation-set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de57d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division completed：\n",
      "train-set（440）→ ./data_origin/train.txt\n",
      "val-set（110）→ ./data_origin/val.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "data_root = os.path.dirname(origin_sequence_dir)\n",
    "train_txt = os.path.join(data_root, 'train.txt')\n",
    "val_txt = os.path.join(data_root, 'val.txt')\n",
    "\n",
    "all_ids = sorted([\n",
    "    f[:-4] for f in os.listdir(origin_sequence_dir)\n",
    "    if f.endswith('.csv') and f.startswith('000')\n",
    "])\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(all_ids)\n",
    "split_idx = int(len(all_ids) * train_ratio)\n",
    "train_ids = sorted(all_ids[:split_idx])\n",
    "val_ids = sorted(all_ids[split_idx:])\n",
    "\n",
    "with open(train_txt, 'w') as f:\n",
    "    f.writelines([id_ + '\\n' for id_ in train_ids])\n",
    "with open(val_txt, 'w') as f:\n",
    "    f.writelines([id_ + '\\n' for id_ in val_ids])\n",
    "\n",
    "print(f\"division completed：\\ntrain-set（{len(train_ids)}）→ {train_txt}\\nval-set（{len(val_ids)}）→ {val_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90dd61f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division completed：\n",
      "  - baby-food       : train=84 , val=21 , total=105\n",
      "  - bottle          : train=25 , val=7  , total=32\n",
      "  - breast          : train=0  , val=1  , total=1\n",
      "  - crawl           : train=22 , val=6  , total=28\n",
      "  - face-down       : train=11 , val=3  , total=14\n",
      "  - face-side       : train=14 , val=4  , total=18\n",
      "  - face-up         : train=9  , val=3  , total=12\n",
      "  - hold-horizontal : train=50 , val=13 , total=63\n",
      "  - hold-vertical   : train=56 , val=15 , total=71\n",
      "  - piggyback       : train=35 , val=9  , total=44\n",
      "  - roll-over       : train=33 , val=9  , total=42\n",
      "  - sit-floor       : train=41 , val=11 , total=52\n",
      "  - sit-high-chair  : train=8  , val=3  , total=11\n",
      "  - sit-low-chair   : train=4  , val=1  , total=5\n",
      "  - stand           : train=11 , val=3  , total=14\n",
      "  - walk            : train=30 , val=8  , total=38\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "data_root = os.path.dirname(origin_sequence_dir)\n",
    "train_txt = os.path.join(data_root, 'train.txt')\n",
    "val_txt = os.path.join(data_root, 'val.txt')\n",
    "train_ratio = 0.8\n",
    "\n",
    "# action -> [sample_id, ...]\n",
    "action2ids = defaultdict(list)\n",
    "\n",
    "for label_file in os.listdir(origin_label_dir):\n",
    "    if not label_file.endswith('_label.csv'):\n",
    "        continue\n",
    "\n",
    "    sample_id = label_file.replace('_label.csv', '')\n",
    "    label_path = os.path.join(origin_label_dir, label_file)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(label_path)\n",
    "        if 'action' not in df.columns:\n",
    "            continue\n",
    "        action = df.loc[0, 'action']\n",
    "        action2ids[action].append(sample_id)\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] 跳过出错文件: {label_file} ({e})\")\n",
    "\n",
    "\n",
    "train_ids, val_ids = [], []\n",
    "\n",
    "random.seed(42)\n",
    "for action, ids in action2ids.items():\n",
    "    random.shuffle(ids)\n",
    "    split_idx = int(len(ids) * train_ratio)\n",
    "    train_ids.extend(ids[:split_idx])\n",
    "    val_ids.extend(ids[split_idx:])\n",
    "\n",
    "train_ids.sort()\n",
    "val_ids.sort()\n",
    "\n",
    "with open(train_txt, 'w') as f:\n",
    "    f.writelines([id_ + '\\n' for id_ in sorted(train_ids)])\n",
    "with open(val_txt, 'w') as f:\n",
    "    f.writelines([id_ + '\\n' for id_ in sorted(val_ids)])\n",
    "\n",
    "\n",
    "print(f\"division completed：\")\n",
    "for action in sorted(action2ids.keys()):\n",
    "    total = len(action2ids[action])\n",
    "    train_count = len([i for i in train_ids if i in action2ids[action]])\n",
    "    val_count = len([i for i in val_ids if i in action2ids[action]])\n",
    "    print(f\"  - {action:<15} : train={train_count:<3}, val={val_count:<3}, total={total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d4766",
   "metadata": {},
   "source": [
    "# Generate the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "100f3a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)模拟动作：婴儿在地板上爬行(action=\"crawl\")\n",
      "(2)统计数据：\n",
      "[\n",
      "  {\n",
      "    \"id\": \"000031\",\n",
      "    \"accel_x\": \"0.728625343±0.072465999\",\n",
      "    \"accel_y\": \"0.111381894±0.190818416\",\n",
      "    \"accel_z\": \"0.679442633±0.103526492\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000044\",\n",
      "    \"accel_x\": \"0.386851241±0.168611129\",\n",
      "    \"accel_y\": \"-0.390523771±0.145563532\",\n",
      "    \"accel_z\": \"0.747573480±0.161554191\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000050\",\n",
      "    \"accel_x\": \"0.581115723±0.101970513\",\n",
      "    \"accel_y\": \"-0.092643738±0.278959798\",\n",
      "    \"accel_z\": \"0.658294678±0.084085999\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000052\",\n",
      "    \"accel_x\": \"0.417088100±0.194192399\",\n",
      "    \"accel_y\": \"-0.506596157±0.128982291\",\n",
      "    \"accel_z\": \"0.653477260±0.251737379\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000054\",\n",
      "    \"accel_x\": \"0.484242031±0.173512301\",\n",
      "    \"accel_y\": \"-0.465523856±0.123724930\",\n",
      "    \"accel_z\": \"0.586447579±0.161683279\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000075\",\n",
      "    \"accel_x\": \"0.843816848±0.119261591\",\n",
      "    \"accel_y\": \"-0.410522461±0.106638037\",\n",
      "    \"accel_z\": \"0.347943987±0.143504045\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000078\",\n",
      "    \"accel_x\": \"0.931058017±0.078189653\",\n",
      "    \"accel_y\": \"-0.201896529±0.152231362\",\n",
      "    \"accel_z\": \"0.273811479±0.168078028\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000088\",\n",
      "    \"accel_x\": \"0.646399865±0.143685590\",\n",
      "    \"accel_y\": \"-0.526747484±0.102412470\",\n",
      "    \"accel_z\": \"0.502300556±0.110515291\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000090\",\n",
      "    \"accel_x\": \"0.876403809±0.141206240\",\n",
      "    \"accel_y\": \"-0.007675171±0.192150293\",\n",
      "    \"accel_z\": \"0.258590698±0.262086707\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000093\",\n",
      "    \"accel_x\": \"0.764687278±0.123858231\",\n",
      "    \"accel_y\": \"-0.362404563±0.215196396\",\n",
      "    \"accel_z\": \"0.368472013±0.292023949\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000182\",\n",
      "    \"accel_x\": \"0.814195905±0.160722469\",\n",
      "    \"accel_y\": \"0.009201050±0.295012352\",\n",
      "    \"accel_z\": \"0.623811994±0.134119041\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000183\",\n",
      "    \"accel_x\": \"0.820225307±0.059653499\",\n",
      "    \"accel_y\": \"0.050999233±0.049630839\",\n",
      "    \"accel_z\": \"0.495291574±0.115554249\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000370\",\n",
      "    \"accel_x\": \"0.790922942±0.070075664\",\n",
      "    \"accel_y\": \"-0.040651675±0.074306034\",\n",
      "    \"accel_z\": \"0.625226056±0.069204987\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000372\",\n",
      "    \"accel_x\": \"0.662337902±0.186959554\",\n",
      "    \"accel_y\": \"-0.408380553±0.163582707\",\n",
      "    \"accel_z\": \"0.628612430±0.256504396\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000373\",\n",
      "    \"accel_x\": \"0.386851241±0.168611129\",\n",
      "    \"accel_y\": \"-0.390523771±0.145563532\",\n",
      "    \"accel_z\": \"0.747573480±0.161554191\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000374\",\n",
      "    \"accel_x\": \"0.502109104±0.181558630\",\n",
      "    \"accel_y\": \"-0.399229988±0.229985686\",\n",
      "    \"accel_z\": \"0.620576404±0.241900421\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000376\",\n",
      "    \"accel_x\": \"0.753540039±0.105949501\",\n",
      "    \"accel_y\": \"-0.464128767±0.088733843\",\n",
      "    \"accel_z\": \"0.434814453±0.091237113\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000377\",\n",
      "    \"accel_x\": \"0.794478353±0.135818328\",\n",
      "    \"accel_y\": \"-0.267810059±0.260581005\",\n",
      "    \"accel_z\": \"0.339170329±0.284210788\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000379\",\n",
      "    \"accel_x\": \"0.730239868±0.070957407\",\n",
      "    \"accel_y\": \"-0.432746040±0.165493121\",\n",
      "    \"accel_z\": \"0.572565715±0.085569221\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000380\",\n",
      "    \"accel_x\": \"0.815401786±0.145421401\",\n",
      "    \"accel_y\": \"0.017560687±0.264265618\",\n",
      "    \"accel_z\": \"0.598107910±0.139145339\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000381\",\n",
      "    \"accel_x\": \"0.746955024±0.108333787\",\n",
      "    \"accel_y\": \"0.123147470±0.201162711\",\n",
      "    \"accel_z\": \"0.655672427±0.128441517\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"000382\",\n",
      "    \"accel_x\": \"0.734315202±0.107030882\",\n",
      "    \"accel_y\": \"-0.094293543±0.220049192\",\n",
      "    \"accel_z\": \"0.659336812±0.128413088\"\n",
      "  }\n",
      "]\n",
      "请参考以上信息生成符合要求的数据。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "target_action = 'crawl'\n",
    "explanation = '婴儿在地板上爬行'\n",
    "\n",
    "train_txt_path = './data_origin/train.txt'\n",
    "with open(train_txt_path, 'r') as f:\n",
    "    train_ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "result_list = []\n",
    "for seq_id in train_ids:\n",
    "    label_path = os.path.join(origin_label_dir, f'{seq_id}_label.csv')\n",
    "    seq_path = os.path.join(origin_sequence_dir, f'{seq_id}.csv')\n",
    "\n",
    "    if not os.path.exists(label_path) or not os.path.exists(seq_path):\n",
    "        continue\n",
    "\n",
    "    label_df = pd.read_csv(label_path)\n",
    "    if 'action' not in label_df.columns or label_df.iloc[0]['action'] != target_action:\n",
    "        continue\n",
    "\n",
    "    data_df = pd.read_csv(seq_path)\n",
    "    if not all(col in data_df.columns for col in ['accel_x', 'accel_y', 'accel_z']):\n",
    "        continue\n",
    "\n",
    "    # calculate mean and std\n",
    "    mean_series = data_df.mean()\n",
    "    std_series = data_df.std()\n",
    "\n",
    "    sample_stat = {\n",
    "        \"id\": seq_id,\n",
    "        \"accel_x\": f\"{mean_series['accel_x']:.9f}±{std_series['accel_x']:.9f}\",\n",
    "        \"accel_y\": f\"{mean_series['accel_y']:.9f}±{std_series['accel_y']:.9f}\",\n",
    "        \"accel_z\": f\"{mean_series['accel_z']:.9f}±{std_series['accel_z']:.9f}\"\n",
    "    }\n",
    "\n",
    "    result_list.append(sample_stat)\n",
    "\n",
    "# 打印\n",
    "print(f\"(1)模拟动作：{explanation}(action=\\\"{target_action}\\\")\")\n",
    "print(f\"(2)统计数据：\\n{json.dumps(result_list[:], indent=2, ensure_ascii=False)}\")\n",
    "print(\"请参考以上信息生成符合要求的数据。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed41824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate blank csv files\n",
    "\n",
    "num_files = 150\n",
    "\n",
    "# ensure dir exist\n",
    "os.makedirs(aug_sequence_dir, exist_ok=True)\n",
    "os.makedirs(aug_label_dir, exist_ok=True)\n",
    "\n",
    "for i in range(num_files):\n",
    "    # with the name of Axxxxx.csv\n",
    "    file_id = f\"A{i:05d}\"\n",
    "    sequence_path = os.path.join(aug_sequence_dir, f\"{file_id}.csv\")\n",
    "    label_path = os.path.join(aug_label_dir, f\"{file_id}_label.csv\")\n",
    "    # blank sequence file\n",
    "    if not os.path.exists(sequence_path):\n",
    "        with open(sequence_path, 'w') as f:\n",
    "            pass\n",
    "    # blank label file\n",
    "    if not os.path.exists(label_path):\n",
    "        with open(label_path, 'w') as f:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
